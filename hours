import pandas as pd
import warnings


def join_transactions_by_minute_optimal(
    dfa,
    dfb,
    dt_a,
    dt_b,
    montant_col,
    tz_a="UTC",
    tz_b="Europe/Paris",
    handle_ambiguous="infer",  # 'infer', 'NaT', ou 'raise'
    handle_duplicates="keep_all"  # 'keep_all', 'keep_first', 'flag'
):
    """
    Jointure fiable des transactions √† la minute pr√®s avec gestion DST robuste.
    
    Param√®tres:
    -----------
    dfa, dfb : DataFrame
        DataFrames contenant les transactions
    dt_a, dt_b : str
        Noms des colonnes datetime
    montant_col : str
        Nom de la colonne montant (doit exister dans les deux DataFrames)
    tz_a, tz_b : str
        Fuseaux horaires des donn√©es sources
    handle_ambiguous : str
        - 'infer' : tente de d√©duire l'heure correcte (recommand√©)
        - 'NaT' : exclut les heures ambigu√´s
        - 'raise' : l√®ve une exception
    handle_duplicates : str
        - 'keep_all' : garde tous les duplicatas de jointure
        - 'keep_first' : garde uniquement la premi√®re occurrence
        - 'flag' : ajoute un flag pour identifier les duplicatas
    
    Retourne:
    ---------
    DataFrame joint avec m√©tadonn√©es de jointure
    """
    # Validation des inputs
    if montant_col not in dfa.columns or montant_col not in dfb.columns:
        raise ValueError(f"La colonne '{montant_col}' doit exister dans les deux DataFrames")
    
    if dt_a not in dfa.columns or dt_b not in dfb.columns:
        raise ValueError(f"Les colonnes de dates '{dt_a}' et '{dt_b}' doivent exister")
    
    # Copies pour √©viter les modifications
    dfa = dfa.copy()
    dfb = dfb.copy()
    
    # Fonction interne pour normalisation timezone
    def safe_tz_convert(series, tz, name="datetime"):
        """Convertit une s√©rie en UTC avec gestion DST robuste."""
        series = pd.to_datetime(series)
        
        # Si d√©j√† timezone-aware, convertir directement
        if series.dt.tz is not None:
            return series.dt.tz_convert("UTC")
        
        # Sinon, localiser puis convertir
        try:
            if handle_ambiguous == "infer":
                # Essayer 'infer', sinon fallback sur premi√®re occurrence (False)
                try:
                    series_tz = series.dt.tz_localize(
                        tz,
                        nonexistent="shift_forward",
                        ambiguous="infer"
                    )
                except:
                    # Si 'infer' √©choue, assumer premi√®re occurrence du DST fall back
                    series_tz = series.dt.tz_localize(
                        tz,
                        nonexistent="shift_forward",
                        ambiguous=False  # False = premi√®re occurrence (avant le recul)
                    )
            elif handle_ambiguous == "NaT":
                series_tz = series.dt.tz_localize(
                    tz,
                    nonexistent="shift_forward",
                    ambiguous="NaT"
                )
            else:  # 'raise'
                series_tz = series.dt.tz_localize(
                    tz,
                    nonexistent="shift_forward",
                    ambiguous="raise"
                )
            
            return series_tz.dt.tz_convert("UTC")
            
        except Exception as e:
            raise ValueError(f"Erreur de conversion timezone pour {name}: {e}")
    
    # Conversion en UTC
    dfa["dt_utc"] = safe_tz_convert(dfa[dt_a], tz_a, name=f"dfa[{dt_a}]")
    dfb["dt_utc"] = safe_tz_convert(dfb[dt_b], tz_b, name=f"dfb[{dt_b}]")
    
    # Supprimer les NaT si pr√©sents
    n_nat_a = dfa["dt_utc"].isna().sum()
    n_nat_b = dfb["dt_utc"].isna().sum()
    
    if n_nat_a > 0 or n_nat_b > 0:
        warnings.warn(
            f"Suppression de {n_nat_a} lignes (dfa) et {n_nat_b} lignes (dfb) "
            f"avec dates ambigu√´s/invalides (NaT)",
            UserWarning
        )
        dfa = dfa.dropna(subset=["dt_utc"])
        dfb = dfb.dropna(subset=["dt_utc"])
    
    # Cr√©er la cl√© de jointure √† la minute (ignorer secondes/ms)
    dfa["dt_minute"] = dfa["dt_utc"].dt.floor("min")
    dfb["dt_minute"] = dfb["dt_utc"].dt.floor("min")
    
    # Ajouter des index pour tra√ßabilit√©
    dfa["_idx_a"] = range(len(dfa))
    dfb["_idx_b"] = range(len(dfb))
    
    # Jointure sur minute + montant
    df_join = dfa.merge(
        dfb,
        on=["dt_minute", montant_col],
        how="inner",
        suffixes=("_a", "_b")
    )
    
    # Gestion des duplicatas de jointure
    if handle_duplicates == "keep_first":
        df_join = df_join.drop_duplicates(subset=["_idx_a"], keep="first")
    elif handle_duplicates == "flag":
        df_join["is_duplicate_match"] = df_join.duplicated(subset=["_idx_a"], keep=False)
    
    # Calculer le delta en secondes (optionnel, pour analyse)
    df_join["delta_seconds"] = (
        df_join["dt_utc_b"] - df_join["dt_utc_a"]
    ).dt.total_seconds()
    
    # Statistiques de jointure
    stats = {
        "n_input_a": len(dfa),
        "n_input_b": len(dfb),
        "n_matched": len(df_join),
        "n_unmatched_a": len(dfa) - df_join["_idx_a"].nunique(),
        "n_unmatched_b": len(dfb) - df_join["_idx_b"].nunique(),
    }
    
    # Stocker les stats dans les attributs (si besoin)
    df_join.attrs["join_stats"] = stats
    
    return df_join


# ==============================================================================
# TESTS AVEC DONN√âES R√âELLES
# ==============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("TEST DE LA FONCTION DE JOINTURE")
    print("=" * 80)
    
    # Cr√©ation des DataFrames de test
    dfa = pd.DataFrame({
        "id_trx": [1, 2, 3, 4, 5],
        "date_heure_a": [
            "2024-02-15 10:00:12",  # journ√©e normale
            "2024-03-31 01:30:45",  # avant heure inexistante DST (spring forward)
            "2024-10-27 01:30:30",  # heure dupliqu√©e DST (fall back)
            "2024-07-01 14:45:59",  # √©t√©
            "2024-07-01 14:45:10"   # m√™me minute que ligne pr√©c√©dente
        ],
        "montant": [1000, 500, 750, 200, 200]
    })
    
    dfb = pd.DataFrame({
        "ref_trx": ["A", "B", "C", "D", "E"],
        "date_heure_b": [
            "2024-02-15 11:00:05",  # correspond √† UTC 10:00 (Paris UTC+1)
            "2024-03-31 03:30:00",  # heure apr√®s spring forward (02:30 n'existe pas)
            "2024-10-27 02:30:00",  # heure dupliqu√©e (peut √™tre 00:30 ou 01:30 UTC)
            "2024-07-01 16:45:00",  # √©t√© UTC+2
            "2024-07-01 16:45:30"   # m√™me minute que ligne pr√©c√©dente
        ],
        "montant": [1000, 500, 750, 200, 200]
    })
    
    print("\nüìä DataFrame A (source UTC):")
    print(dfa)
    
    print("\nüìä DataFrame B (source Europe/Paris):")
    print(dfb)
    
    print("\n" + "=" * 80)
    print("JOINTURE AVEC GESTION DST 'infer'")
    print("=" * 80)
    
    # Ex√©cution de la jointure
    df_join = join_transactions_by_minute_optimal(
        dfa=dfa,
        dfb=dfb,
        dt_a="date_heure_a",
        dt_b="date_heure_b",
        montant_col="montant",
        tz_a="UTC",
        tz_b="Europe/Paris",
        handle_ambiguous="infer",
        handle_duplicates="flag"
    )
    
    print("\n‚úÖ R√©sultat de la jointure:")
    print(df_join[[
        "id_trx", "ref_trx", "montant",
        "date_heure_a", "date_heure_b",
        "dt_minute", "delta_seconds", "is_duplicate_match"
    ]])
    
    print("\nüìà Statistiques de jointure:")
    for key, value in df_join.attrs.get("join_stats", {}).items():
        print(f"  {key}: {value}")
    
    # Test avec NaT pour heures ambigu√´s
    print("\n" + "=" * 80)
    print("JOINTURE AVEC GESTION DST 'NaT' (exclut heures ambigu√´s)")
    print("=" * 80)
    
    df_join_nat = join_transactions_by_minute_optimal(
        dfa=dfa,
        dfb=dfb,
        dt_a="date_heure_a",
        dt_b="date_heure_b",
        montant_col="montant",
        tz_a="UTC",
        tz_b="Europe/Paris",
        handle_ambiguous="NaT",
        handle_duplicates="keep_all"
    )
    
    print("\n‚úÖ R√©sultat avec exclusion ambigu√´s:")
    print(df_join_nat[[
        "id_trx", "ref_trx", "montant",
        "dt_minute", "delta_seconds"
    ]])
    
    # Analyse des non-match√©s
    print("\n" + "=" * 80)
    print("ANALYSE DES NON-MATCH√âS")
    print("=" * 80)
    
    matched_a = df_join["id_trx"].unique()
    matched_b = df_join["ref_trx"].unique()
    
    unmatched_a = dfa[~dfa["id_trx"].isin(matched_a)]
    unmatched_b = dfb[~dfb["ref_trx"].isin(matched_b)]
    
    if len(unmatched_a) > 0:
        print("\n‚ö†Ô∏è  Transactions A non match√©es:")
        print(unmatched_a)
    else:
        print("\n‚úÖ Toutes les transactions A ont √©t√© match√©es")
    
    if len(unmatched_b) > 0:
        print("\n‚ö†Ô∏è  Transactions B non match√©es:")
        print(unmatched_b)
    else:
        print("\n‚úÖ Toutes les transactions B ont √©t√© match√©es")
    
    print("\n" + "=" * 80)
    print("TEST COMPL√âT√â ‚úÖ")
    print("=" * 80)
    
    
    
#########
df_join = join_transactions_by_minute_optimal(
    dfa=dfa,
    dfb=dfb,
    dt_a="date_utc",
    dt_b="date_casablanca",
    montant_col="montant",
    tz_a="UTC",
    tz_b="Africa/Casablanca"  # ‚Üê Simplement √ßa !
)
